{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d60f24",
   "metadata": {},
   "source": [
    " **(Batch) Gradient Descent**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d39f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gradient():\n",
    "    X = data_input\n",
    "    Y = labels\n",
    "    m = X.shape[1]  # Number of training examples\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "    # Forward propagation\n",
    "        a, caches = forward_propagation(X, parameters)\n",
    "    # Compute cost\n",
    "        cost_total = compute_cost(a, Y)  # Cost for m training examples\n",
    "    # Backward propagation\n",
    "        grads = backward_propagation(a, caches, parameters)\n",
    "    # Update parameters\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e878820",
   "metadata": {},
   "source": [
    "- **Stochastic Gradient Descent**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f4e753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic():\n",
    "    X = data_input\n",
    "    Y = labels\n",
    "    m = X.shape[1]  # Number of training examples\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "    for i in range(0, num_iterations):\n",
    "        cost_total = 0\n",
    "        for j in range(0, m):\n",
    "        # Forward propagation\n",
    "            a, caches = forward_propagation(X[:,j], parameters)\n",
    "        # Compute cost\n",
    "            cost_total += compute_cost(a, Y[:,j])  # Cost for one training example\n",
    "        # Backward propagation\n",
    "            grads = backward_propagation(a, caches, parameters)\n",
    "        # Update parameters\n",
    "            parameters = update_parameters(parameters, grads)\n",
    "    # Compute average cost\n",
    "        cost_avg = cost_total / m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974468b3",
   "metadata": {},
   "source": [
    "**mini batch gradient descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffbac0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: random_mini_batches\n",
    "\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    \n",
    "    inc = mini_batch_size\n",
    "\n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "\n",
    "        mini_batch_X =  shuffled_X[:, k*mini_batch_size :(k+1)*mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k*mini_batch_size :(k+1)*mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    if m % mini_batch_size != 0:\n",
    "\n",
    "        mini_batch_X = shuffled_X[:,(mini_batch_size*num_complete_minibatches):m ]\n",
    "        mini_batch_Y = shuffled_Y[:,(mini_batch_size*num_complete_minibatches):m ]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e974d0",
   "metadata": {},
   "source": [
    "### update_parameters_with_momentum\n",
    "\n",
    "\n",
    "\n",
    "$$ \\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta v_{dW^{[l]}} + (1 - \\beta) dW^{[l]} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha v_{dW^{[l]}}\n",
    "\\end{cases}\\tag{3}$$\n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{db^{[l]}} = \\beta v_{db^{[l]}} + (1 - \\beta) db^{[l]} \\\\\n",
    "b^{[l]} = b^{[l]} - \\alpha v_{db^{[l]}} \n",
    "\\end{cases}\\tag{4}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7786d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters_with_momentum\n",
    "\n",
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "\n",
    "\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "    \n",
    "        v[\"dW\" + str(l)] = beta*v[\"dW\" + str(l)]+(1-beta)*(grads['dW' + str(l)])\n",
    "        v[\"db\" + str(l)] = beta*v[\"db\" + str(l)]+(1-beta)*(grads['db' + str(l)])\n",
    "        parameters[\"W\" + str(l)] =parameters[\"W\" + str(l)]-learning_rate*v[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] =parameters[\"b\" + str(l)]-learning_rate*v[\"db\" + str(l)]\n",
    "        \n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338cd7a",
   "metadata": {},
   "source": [
    "### update_parameters_with_adam\n",
    "\n",
    "Now, implement the parameters update with Adam. Recall the general update rule is, for $l = 1, ..., L$: \n",
    "\n",
    "$$\\begin{cases}\n",
    "v_{dW^{[l]}} = \\beta_1 v_{dW^{[l]}} + (1 - \\beta_1) \\frac{\\partial \\mathcal{J} }{ \\partial W^{[l]} } \\\\\n",
    "v^{corrected}_{dW^{[l]}} = \\frac{v_{dW^{[l]}}}{1 - (\\beta_1)^t} \\\\\n",
    "s_{dW^{[l]}} = \\beta_2 s_{dW^{[l]}} + (1 - \\beta_2) (\\frac{\\partial \\mathcal{J} }{\\partial W^{[l]} })^2 \\\\\n",
    "s^{corrected}_{dW^{[l]}} = \\frac{s_{dW^{[l]}}}{1 - (\\beta_2)^t} \\\\\n",
    "W^{[l]} = W^{[l]} - \\alpha \\frac{v^{corrected}_{dW^{[l]}}}{\\sqrt{s^{corrected}_{dW^{[l]}}} + \\varepsilon}\n",
    "\\end{cases}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters_with_adam\n",
    "\n",
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    " \n",
    "        v[\"dW\" + str(l)] = beta1*v[\"dW\" + str(l)]+(1-beta1)*(grads['dW' + str(l)])\n",
    "        v[\"db\" + str(l)] = beta1*v[\"db\" + str(l)]+(1-beta1)*(grads['db' + str(l)])   \n",
    "\n",
    "        v_corrected[\"dW\" + str(l)] =v[\"dW\" + str(l)]/(1 - math.pow(beta1,t))\n",
    "        v_corrected[\"db\" + str(l)] =v[\"db\" + str(l)]/(1 - math.pow(beta1,t))\n",
    "\n",
    "         \n",
    "        s[\"dW\" + str(l)] = beta2*s[\"dW\" + str(l)] +(1-beta2)*(grads['dW' + str(l)])**2\n",
    "        s[\"db\" + str(l)] = beta2*s[\"db\" + str(l)] +(1-beta2)*(grads['db' + str(l)])**2\n",
    "\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l)] =s[\"dW\" + str(l)]/(1 - math.pow(beta2,t))\n",
    "        s_corrected[\"db\" + str(l)] =s[\"db\" + str(l)]/(1 - math.pow(beta2,t))\n",
    "        \n",
    "\n",
    "        parameters[\"W\" + str(l)] =parameters[\"W\" + str(l)] - learning_rate* v_corrected[\"dW\" + str(l)]/(np.sqrt(s_corrected[\"dW\" + str(l)])+epsilon)\n",
    "        parameters[\"b\" + str(l)] =parameters[\"b\" + str(l)] - learning_rate* v_corrected[\"db\" + str(l)]/(np.sqrt(s_corrected[\"db\" + str(l)])+epsilon)\n",
    "\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05cc2be",
   "metadata": {},
   "source": [
    "### Learning Rate Decay and Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894026d4",
   "metadata": {},
   "source": [
    "$$\\alpha = \\frac{1}{1 + decayRate \\times epochNumber} \\alpha_{0}$$\n",
    "\n",
    "\n",
    "$$\\alpha = \\frac{1}{1 + decayRate \\times \\lfloor\\frac{epochNum}{timeInterval}\\rfloor} \\alpha_{0}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a8876",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_learning_rate(learning_rate0, epoch_num, decay_rate):\n",
    " \n",
    "    learning_rate=learning_rate0/(1+epoch_num*decay_rate)\n",
    "    \n",
    "    return learning_rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
