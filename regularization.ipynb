{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2664a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A,Y):\n",
    "    cost=np.mean(-(Y*np.log(A)+(1-Y)*np.log(1-A)))\n",
    "    cost=np.squeeze(cost)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba18c67",
   "metadata": {},
   "source": [
    "\n",
    "The standard way to avoid overfitting is called **L2 regularization**. It consists of appropriately modifying your cost function, from:\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Let's modify your cost and observe the consequences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52c10325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(A,Y,parameters,lam):\n",
    "    m=Y.shape[1]\n",
    "    cross_entropy_cost=compute_cost(A,Y)\n",
    "    L=len(parameters)//2\n",
    "    L2_regularization_cost=0\n",
    "    for i in range(1,L+1):\n",
    "        L2_regularization_cost += (lambd/(m*2))*(np.sum(np.square(parameters[\"W\"+str(i)])))\n",
    "    cost = cross_entropy_cost + L2_regularization_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dd79180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation==\"sigmoid\":\n",
    "        dz=sigmoid_backward(dA,activation_cache)\n",
    "       \n",
    "    if activation==\"relu\":\n",
    "        dz=relu_backward(dA,activation_cache)\n",
    "\n",
    "    A_prev, W, b = linear_cache\n",
    "    m=A_prev.shape[1]\n",
    "    dW = (1/m)*np.dot(dz,A_prev.T)\n",
    "    db = (1/m)*np.sum(dz,keepdims=True,axis=1)\n",
    "    dA_prev = np.dot(W.T,dz)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6011f",
   "metadata": {},
   "source": [
    "for calculate backward propagation with regularization for 3 layers:<br>\n",
    "    ($\\frac{d}{dW} ( \\frac{1}{2}\\frac{\\lambda}{m}  W^2) = \\frac{\\lambda}{m} W$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fe99b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: backward_propagation_with_regularization\n",
    "\n",
    "def backward_propagation_with_regularization(X, Y, cache, lambd):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    (Z1, A1, W1, b1, Z2, A2, W2, b2, Z3, A3, W3, b3) = cache\n",
    "    \n",
    "    dZ3 = A3 - Y\n",
    "   \n",
    "    dW3 = 1./m * np.dot(dZ3, A2.T) +(lambd/m)*W3\n",
    "    \n",
    "    db3 = 1. / m * np.sum(dZ3, axis=1, keepdims=True)\n",
    "    \n",
    "    dA2 = np.dot(W3.T, dZ3)\n",
    "    dZ2 = np.multiply(dA2, np.int64(A2 > 0))\n",
    "\n",
    "    \n",
    "    dW2 = 1./m * np.dot(dZ2, A1.T) + (lambd/m)*W2\n",
    "    db2 = 1. / m * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    \n",
    "    dA1 = np.dot(W2.T, dZ2)\n",
    "    dZ1 = np.multiply(dA1, np.int64(A1 > 0))\n",
    "\n",
    "    \n",
    "    dW1 = 1./m * np.dot(dZ1, X.T)+(lambd/m)*W1\n",
    "    db1 = 1. / m * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    gradients = {\"dZ3\": dZ3, \"dW3\": dW3, \"db3\": db3,\"dA2\": dA2,\n",
    "                 \"dZ2\": dZ2, \"dW2\": dW2, \"db2\": db2, \"dA1\": dA1, \n",
    "                 \"dZ1\": dZ1, \"dW1\": dW1, \"db1\": db1}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61506b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward_with_regularization(dA, cache, activation,lamb):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation==\"sigmoid\":\n",
    "        dz=sigmoid_backward(dA,activation_cache)\n",
    "       \n",
    "    if activation==\"relu\":\n",
    "        dz=relu_backward(dA,activation_cache)\n",
    "\n",
    "    A_prev, W, b = linear_cache\n",
    "    m=A_prev.shape[1]\n",
    "    dW = (1/m)*np.dot(dz,A_prev.T)+(lamb/m)*W\n",
    "    db = (1/m)*np.sum(dz,keepdims=True,axis=1)\n",
    "    dA_prev = np.dot(W.T,dz)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90b538e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches,regularization=True):\n",
    "\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    if regularization:\n",
    "        current_cache = caches[L-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward_with_regularization(dAL, current_cache, activation = \"sigmoid\")\n",
    "        grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(L)] = dW_temp\n",
    "        grads[\"db\" + str(L)] =db_temp\n",
    "        for l in reversed(range(L-1)):\n",
    "        \n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward_with_regularization(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l+1)] = dW_temp\n",
    "            grads[\"db\" + str(l+1)] =db_temp\n",
    "        \n",
    "    else:\n",
    "        current_cache = caches[L-1]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "        grads[\"dA\" + str(L-1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(L)] = dW_temp\n",
    "        grads[\"db\" + str(L)] =db_temp\n",
    "        for l in reversed(range(L-1)):\n",
    "        \n",
    "            current_cache = caches[l]\n",
    "            dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "            grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "            grads[\"dW\" + str(l+1)] = dW_temp\n",
    "            grads[\"db\" + str(l+1)] =db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aee8515",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
